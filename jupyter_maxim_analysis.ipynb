{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "KfXz9WySWmeb",
        "8r_WME9cUs4R",
        "h1wNL2GeW8sH",
        "LrZYkJKzXq-H",
        "LtlGUZXXaVWc",
        "JdVrFvdDafo_",
        "HfV7HmPqZexX",
        "tCPCQZd7m0Vf",
        "-ewCMnOXDF09",
        "5QHI0jfgEcfC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Package"
      ],
      "metadata": {
        "id": "jvN84C-7WOuU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVOJOWeaS163"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Nuvantim/Maxim_sentiment_model/refs/heads/main/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -r requirements.txt"
      ],
      "metadata": {
        "id": "KFzhiQndUVGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import emoji\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import joblib\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from gensim.models import FastText\n",
        "from tqdm import tqdm\n",
        "from google_play_scraper import Sort,reviews_all\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "9xQABrWVWXlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "c3ZQ4Z5_Uz9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping Maxim app reviews from Google Play Store"
      ],
      "metadata": {
        "id": "KfXz9WySWmeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google_play_scraper import Sort, reviews_all\n",
        "import pandas as pd\n",
        "\n",
        "result = reviews_all(\n",
        "    'com.taxsee.taxsee',\n",
        "    lang='id'\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(result)\n",
        "df = df[['userName','content','score','at']]\n",
        "df = df.rename(columns={'content':'review','score':'rating'})\n",
        "\n",
        "df['at'] = pd.to_datetime(df['at'])\n",
        "start_date = pd.to_datetime('2020-01-01')\n",
        "end_date = pd.to_datetime('2024-12-31')\n",
        "filtered_df = df[(df['at'] >= start_date) & (df['at'] <= end_date)]\n",
        "\n",
        "filtered_df.to_csv('maxim_gplay.csv',index=False,escapechar='\\\\')"
      ],
      "metadata": {
        "id": "fNNlG7AuWiW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data"
      ],
      "metadata": {
        "id": "8r_WME9cUs4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Nuvantim/Maxim_sentiment_model/raw/refs/heads/main/sentiment_maxim.7z\n",
        "!7z x sentiment_maxim.7z\n",
        "!rm *.7z"
      ],
      "metadata": {
        "id": "9Kv27VeffGQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import data"
      ],
      "metadata": {
        "id": "h1wNL2GeW8sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('sentiment_maxim_gplay.csv')"
      ],
      "metadata": {
        "id": "Lurmg1CqW7z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "IbzmgZ37XgwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "jj2t7tT7Xi5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering data"
      ],
      "metadata": {
        "id": "LrZYkJKzXq-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove emoji & special characters"
      ],
      "metadata": {
        "id": "CptLrkueX5fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['review'] = (\n",
        "    data['review']\n",
        "    .dropna()\n",
        "    .apply(lambda s: emoji.replace_emoji(str(s), ''))\n",
        "    .str.replace('[^a-zA-Z0-9]', ' ', regex=True)\n",
        "    .replace('', np.nan)\n",
        ")"
      ],
      "metadata": {
        "id": "W_DSd-5DXvkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove missing value"
      ],
      "metadata": {
        "id": "Cg3X5KbMYNnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna(subset=['review'])"
      ],
      "metadata": {
        "id": "tsacTBi1YLiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "A6h9IkGgYv53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove entries labeled as \"NETRAL\""
      ],
      "metadata": {
        "id": "jTxGniMcYz3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[data['label'] != 'NETRAL']"
      ],
      "metadata": {
        "id": "1-5rrQWzZJ4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "1__Av9p6Atcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word tokenizer"
      ],
      "metadata": {
        "id": "-d52oU13YikR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(['punkt', 'punkt_tab'])\n",
        "data['token'] = data['review'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "UoqNuwgsZWRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "L33Psf8EZa3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowercase tokens"
      ],
      "metadata": {
        "id": "YROtog3cCYoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = data[['review', 'token', 'label','at']]\n",
        "data['token'] = data['token'].apply(lambda tokens: [t.lower() for t in tokens])"
      ],
      "metadata": {
        "id": "oCYL-InACT5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backup Data"
      ],
      "metadata": {
        "id": "m-zsEqM3CqTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv(\"clean_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "Ij5FwyxRCkbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "CnToSKm8aDWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Number of reviews per year"
      ],
      "metadata": {
        "id": "LtlGUZXXaVWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.copy()\n",
        "df['at'] = pd.to_datetime(data['at'])\n",
        "\n",
        "df['tahun'] = df['at'].dt.year\n",
        "\n",
        "df = df[df['label'].isin(['POSITIF', 'NEGATIF'])]\n",
        "\n",
        "count_per_year = df.groupby(['tahun', 'label']).size().unstack(fill_value=0)\n",
        "print(count_per_year)\n",
        "\n",
        "\n",
        "count_per_year.plot(kind='bar', figsize=(10,6))\n",
        "plt.title(\"Number of reviews per year\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Amount\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_LRmx3SJaHn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordCloud"
      ],
      "metadata": {
        "id": "JdVrFvdDafo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = []\n",
        "for tokens in tqdm(df['token'], desc=\"Merging tokens\"):\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "word_freq = Counter(all_tokens)\n",
        "\n",
        "wc = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    background_color='white',\n",
        "    colormap='viridis'\n",
        ").generate_from_frequencies(word_freq)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"WordCloud of Reviews\", fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oHxuik43gskp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Fasttext Model"
      ],
      "metadata": {
        "id": "HfV7HmPqZexX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models"
      ],
      "metadata": {
        "id": "gnLgseLupKmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "model = FastText(\n",
        "    sentences=data['token'],\n",
        "    vector_size=500,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    epochs=20\n",
        ")"
      ],
      "metadata": {
        "id": "CckEl6laZ9D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"models/maxim_fasttext.model\")"
      ],
      "metadata": {
        "id": "GkdsktQ8kcPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "tCPCQZd7m0Vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split features and labels"
      ],
      "metadata": {
        "id": "oKN4Fivjm8Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data = pd.read_csv('clean_data.csv')\n",
        "data['review'] = data['review'].str.lower()\n",
        "\n",
        "X = data['review'].astype(str)\n",
        "y = data['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "Mup67B2gm6FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert reviews to embeddings"
      ],
      "metadata": {
        "id": "lxtsGMi1nCdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model = FastText.load(\"models/maxim_fasttext.model\")"
      ],
      "metadata": {
        "id": "0pzAtxJ_nLL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def review_to_vec(tokens, model):\n",
        "    vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
        "    return np.mean(vecs, axis=0) if len(vecs) > 0 else np.zeros(model.vector_size)\n",
        "\n",
        "X_train_vec = np.array([review_to_vec(text.lower().split(), ft_model) for text in X_train])\n",
        "X_test_vec  = np.array([review_to_vec(text.lower().split(), ft_model) for text in X_test])"
      ],
      "metadata": {
        "id": "VaY0SUFinVy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balance training data"
      ],
      "metadata": {
        "id": "RM7jA-YInrn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_under, y_under = RandomUnderSampler(\n",
        "    sampling_strategy=0.7, random_state=42\n",
        ").fit_resample(X_train_vec, y_train)\n",
        "\n",
        "\n",
        "target_n = Counter(y_under).most_common(1)[0][1]\n",
        "X_res, y_res = SMOTE(\n",
        "    sampling_strategy={c: target_n for c in Counter(y_under)}, random_state=42\n",
        ").fit_resample(X_under, y_under)"
      ],
      "metadata": {
        "id": "xtHJbq36n37U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode labels to integers"
      ],
      "metadata": {
        "id": "cW5pLUhjoGKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y_res_enc = le.fit_transform(y_res)\n",
        "y_test_enc = le.transform(y_test)"
      ],
      "metadata": {
        "id": "6Sy5O-WgoEbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build GRU Model"
      ],
      "metadata": {
        "id": "-ewCMnOXDF09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Data to Pytorch Tensor"
      ],
      "metadata": {
        "id": "PRWka41dDP92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_res, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_res_enc, dtype=torch.long)\n",
        "X_test_tensor  = torch.tensor(X_test_vec, dtype=torch.float32)\n",
        "y_test_tensor  = torch.tensor(y_test_enc, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "J372ImL0DORp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define GRU classifier"
      ],
      "metadata": {
        "id": "PJhIwIiDDec-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
        "        super(GRUClassifier, self).__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size, hidden_size, num_layers,\n",
        "            batch_first=True, dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, 64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "        out, _ = self.gru(x)\n",
        "        out = out[:, -1, :]\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "input_size = X_train_vec.shape[1]\n",
        "hidden_size = 64\n",
        "num_layers = 3\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "model = GRUClassifier(input_size, hidden_size, num_layers, num_classes, dropout=0.2)\n",
        "device = torch.device('cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "a3V07t2cD2SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "o9peVBMjEENe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, patience=2\n",
        ")"
      ],
      "metadata": {
        "id": "TkPzxIvpEL04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GRU model"
      ],
      "metadata": {
        "id": "a1yh34mYETRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "train_losses = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "    scheduler.step(avg_loss)"
      ],
      "metadata": {
        "id": "J4QiJo_YERqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "5QHI0jfgEcfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix"
      ],
      "metadata": {
        "id": "WGmEnpLIGBQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_tensor = X_test_tensor.to(device)\n",
        "    outputs = model(X_test_tensor)\n",
        "    y_pred = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "# Confusion Matrix (visual)\n",
        "cm = confusion_matrix(y_test_enc, y_pred)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_test_enc, y_pred, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "eZ6RHegkEnlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot Loss Function"
      ],
      "metadata": {
        "id": "E-y_fTR9GELJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(range(1, num_epochs+1), train_losses, marker='o', label=\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss per Epoch\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EaLPiVrcHBng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Models"
      ],
      "metadata": {
        "id": "_lajKDshHDCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"./models\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "torch.save(model.state_dict(), f\"{save_dir}/pytorch_model.bin\")\n",
        "\n",
        "config_dict = {\n",
        "    \"model_type\":\"maxim-sentiment-models\",\n",
        "    \"input_size\": input_size,\n",
        "    \"hidden_size\": hidden_size,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"num_classes\": num_classes\n",
        "}\n",
        "\n",
        "with open(f\"{save_dir}/config.json\", \"w\") as f:\n",
        "    json.dump(config_dict, f, indent=2)\n",
        "\n",
        "joblib.dump(le, f\"{save_dir}/label_encoder.pkl\")\n",
        "\n",
        "print(f\"Model saved in {save_dir}\")"
      ],
      "metadata": {
        "id": "Rw2CsUUFHFPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh models"
      ],
      "metadata": {
        "id": "dWw66XlVzh1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}